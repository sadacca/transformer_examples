{"cells":[{"cell_type":"markdown","source":["# Fine tune a classifer with/without accelerate\n","\n","For deploying training jobs on an EC2 or locally, use of the accelerate library can, with few additional lines of code and automatic detection of some of the distributed features of your environment, substantially increase speeds through dividing work among multiple GPUs/TPUs, use of DeepSpeed acceleration, mixed_precision simplification, etc. Here are 2 examples:\n","- first, an example of training a content classifier in a single EC2, single GPU context\n","- second, an example of training a content classifier in a multiple GPU context with Accelerate"],"metadata":{"id":"WWmn3ZMszhq8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"de0HiGP54dHJ"},"outputs":[],"source":["%%capture\n","## capture with jupyter magic to suppress output\n","\n","## install needed libraries\n","!pip install xgboost datasets transformers sentence_transformers nltk accelerate evaluate tqdm deepspeed"]},{"cell_type":"markdown","source":["#### Load a dataset to train\n","*note: this dataset would be loaded in the training script if not run interactively, but for convenience we're loading it before training to only load once*"],"metadata":{"id":"-fD-uCO8zkSC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"U4rQTPs3ff-E"},"outputs":[],"source":["from datasets import load_dataset, ReadInstruction\n","\n","## load a subset of dataset for training and evaluationg\n","dataset = load_dataset(\"UKPLab/toxic_conversations\", split = ReadInstruction('test', to=5, unit='%'))"]},{"cell_type":"markdown","source":["## Single-server Example: model training without Accelerate/DeepSpeed"],"metadata":{"id":"GNXZUEVhzUCu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"J8wivKaY65lu"},"outputs":[],"source":["from accelerate import Accelerator\n","from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler,DataCollatorWithPadding,AutoTokenizer\n","from torch.utils.data import DataLoader\n","import torch\n","import tqdm\n","import pandas as pd\n","import evaluate\n","\n","### EXAMPLE 1 - single-server context\n","##\n","## the below code is aimed at training a content moderation model\n","## on a classification task for a single machine with a single GPU\n","## (e.g. a single EC2 p3.2xlarge)\n","## in a more production-like context this cell would be written to a .py file\n","## to be called at terminal as one training job\n","## (as one of potentially several, or a recurring job)\n","## note: this assumes data was loaded (loaded in above cell for convenience)\n","\n","metric = evaluate.load(\"accuracy\")\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"martin-ha/toxic-comment-model\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"martin-ha/toxic-comment-model\")\n","optimizer = AdamW(model.parameters(), lr=3e-5)\n","\n","tokenized_ds = dataset.map(lambda x: tokenizer(x[\"text\"], truncation=True,padding=True))\n","tokenized_ds = tokenized_ds.remove_columns('label_text')\n","tokenized_ds = tokenized_ds.remove_columns('text')\n","\n","ds_train_test = tokenized_ds.train_test_split(test_size = 0.2)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","train_dataloader = DataLoader(\n","                              ds_train_test['train'],\n","                              shuffle=True,\n","                              batch_size=8,\n","                              collate_fn=data_collator\n",")\n","\n","eval_dataloader = DataLoader(\n","                             ds_train_test['test'],\n","                             batch_size=8,\n","                             collate_fn=data_collator\n","                            )\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","model.to(device)\n","\n","\n","num_epochs = 3\n","num_training_steps = num_epochs * len(train_dataloader)\n","lr_scheduler = get_scheduler(\n","    \"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_training_steps\n",")\n","\n","progress_bar = tqdm.tqdm(range(num_training_steps))\n","\n","model.train()\n","for epoch in range(num_epochs):\n","    for batch in train_dataloader:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        progress_bar.update(1)\n","\n","model.eval()\n","for batch in eval_dataloader:\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","\n","    logits = outputs.logits\n","    predictions = torch.argmax(logits, dim=-1)\n","    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n","\n","metric.compute()"]},{"cell_type":"markdown","source":["## Distributed Example - adding the Accelerate library"],"metadata":{"id":"5eji7Ex6zIba"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jAYy01efSmE3"},"outputs":[],"source":["from accelerate import Accelerator\n","from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler,DataCollatorWithPadding,AutoTokenizer\n","from torch.utils.data import DataLoader\n","import torch\n","import tqdm\n","import pandas as pd\n","import evaluate\n","\n","### EXAMPLE 2 - addition of Accelerate for performance gains in distributed context\n","##\n","## the below code is aimed at training a content moderation model\n","## on a classification task for mulitple machines or a machine with multiple GPUs\n","## (e.g. a larger EC2 like a p3.16xlarge with 8x V100 GPUs)\n","## through addition of a handful of lines of code (noted with comments below)\n","## and removal of a handful of modules of code\n","## in a more production-like context this cell would be written to a .py file\n","## however here the training code is wrapped by a function and can be conveniently called\n","## for protyping and demonstration purposes with a single line of code (in the cell below)\n","## e.g. accelerate.notebook_launcher(accelerated_training_fn, mixed_precision = 'fp16')\n","## note: this assumes data was loaded (loaded in an above cell for convenience)\n","\n","\n","def accelerated_training_fn():\n","\n","    metric = evaluate.load(\"accuracy\")\n","    tokenizer = AutoTokenizer.from_pretrained(\"martin-ha/toxic-comment-model\")\n","    model = AutoModelForSequenceClassification.from_pretrained(\"martin-ha/toxic-comment-model\")\n","    optimizer = AdamW(model.parameters(), lr=3e-5)\n","\n","    tokenized_ds = dataset.map(lambda x: tokenizer(x[\"text\"], truncation=True,padding=True))\n","    tokenized_ds = tokenized_ds.remove_columns('label_text')\n","    tokenized_ds = tokenized_ds.remove_columns('text')\n","\n","    ds_train_test = tokenized_ds.train_test_split(test_size = 0.2)\n","    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","\n","\n","    train_dataloader = DataLoader(\n","                                ds_train_test['train'],\n","                                shuffle=True,\n","                                batch_size=8,\n","                                collate_fn=data_collator\n","    )\n","\n","    eval_dataloader = DataLoader(\n","                                ds_train_test['test'],\n","                                batch_size=8,\n","                                collate_fn=data_collator\n","                                )\n","\n","    accelerator = Accelerator()\n","\n","    # - below removed for accelerate addtion\n","    # - device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","    model.to(device)\n","\n","    # + below added for accelerate\n","    train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n","        train_dataloader, eval_dataloader, model, optimizer\n","    )\n","\n","    num_epochs = 3\n","    num_training_steps = num_epochs * len(train_dataloader)\n","    lr_scheduler = get_scheduler(\n","        \"linear\",\n","        optimizer=optimizer,\n","        num_warmup_steps=0,\n","        num_training_steps=num_training_steps\n","    )\n","\n","    progress_bar = tqdm.tqdm(range(num_training_steps))\n","\n","    model.train()\n","    for epoch in range(num_epochs):\n","        for batch in train_dataloader:\n","            # - below removed for accelerate addtion\n","            #batch = {k: v.to(device) for k, v in batch.items()}\n","            outputs = model(**batch)\n","            loss = outputs.loss\n","\n","            # - below removed for accelerate addtion\n","            #loss.backward()\n","\n","            # + below added for accelerate\n","            accelerator.backward(loss)\n","\n","            optimizer.step()\n","            lr_scheduler.step()\n","            optimizer.zero_grad()\n","            progress_bar.update(1)\n","\n","    model.eval()\n","    for batch in eval_dataloader:\n","        # - below removed for accelerate addtion\n","        #batch = {k: v.to(device) for k, v in batch.items()}\n","        with torch.no_grad():\n","            outputs = model(**batch)\n","\n","        logits = outputs.logits\n","        predictions = torch.argmax(logits, dim=-1)\n","        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n","\n","    metric.compute()"]},{"cell_type":"markdown","metadata":{"id":"_cuZ4U32Wfxf"},"source":["Normally accelerate is launched as a scripted job at the command line - but for convenience here we've specified to run it in a notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"foE__uZkUPs6"},"outputs":[],"source":["from accelerate import notebook_launcher\n","\n","notebook_launcher(accelerated_training_fn, mixed_precision = 'fp16')"]},{"cell_type":"markdown","source":["## Example Config .yml file when executing at the terminal"],"metadata":{"id":"wwG7l3kCy2k8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HnHoeIiRW-I2"},"outputs":[],"source":["# NOTE: if running at the command line, accelerate benefits from generating a config file\n","# with the command \"accelerate config\" followed by answering prompts relevent for your environment\n","\n","# Sample default_config.yaml for a EC2 server with 8GPUs (like a p3.16xlarge):\n","\n","default_yml_p3_16x = \"\"\"\n","compute_environment: LOCAL_MACHINE\n","deepspeed_config:\n","  gradient_accumulation_steps: 1\n","  offload_optimizer_device: none\n","  offload_param_device: none\n","  zero3_init_flag: false\n","  zero_stage: 2\n","distributed_type: DEEPSPEED\n","downcast_bf16: 'no'\n","dynamo_config:\n","  dynamo_backend: INDUCTOR\n","machine_rank: 0\n","main_training_function: main\n","mixed_precision: fp16\n","num_machines: 1\n","num_processes: 8\n","rdzv_backend: static\n","same_network: true\n","tpu_env: []\n","tpu_use_cluster: false\n","tpu_use_sudo: false\n","use_cpu: false\n","\"\"\"\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"INp5OI40UE_w"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1RZPw656gIBq8Gc5r0PRAvTYwKroHzbBp","timestamp":1691694839382}],"gpuType":"V100","authorship_tag":"ABX9TyMDoCqiyOWKiSadI1L5oPdE"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}